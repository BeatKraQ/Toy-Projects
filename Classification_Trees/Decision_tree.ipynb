{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees in Python, From Start to Finish\n",
    "\n",
    "In this lesson we will use **scikit-learn** and **Cost Complexity Pruning** to build this **Classification Tree** (below), which uses continuous and categorical data from the **[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)** to predict whether or not a patient has **[heart disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)**:\n",
    "\n",
    "<img src=\"tree.png\" alt=\"A Classification Tree\" style=\"width: 600px;\">\n",
    "\n",
    "<!-- The **Classification Tree** will use continuous and categorical data from the **[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)** to predict whether or not a patient has **[heart disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)**\n",
    " -->\n",
    " \n",
    "<!-- If you are not already familiary with these terms, check out the **StatQuests:** **[Decision Trees Part 1: Building and Using](https://youtu.be/7VeUPuFGJHk)**, **[Part 2: Feature Selection and Missing Data](https://youtu.be/wpNl-JwwplA)** and **[How to Prune Regression Trees](https://youtu.be/D0efHEJsfHo)**. -->\n",
    "\n",
    "**Classification Trees** are an exceptionally useful machine learning method when you need to to know how the decisions are being made. For example, if you have to justify the predictions to your boss, **Classification Trees** are a good method because each step in the decision making process is easy to understand.\n",
    "\n",
    "In this lesson you will learn about...\n",
    "\n",
    "- **[Importing the Data From a File](#download-the-data)**\n",
    "\n",
    "- **[Missing Data](#identify-and-deal-with-missing-data)**\n",
    "    - Identifying Missing Data\n",
    "    - Dealing with Missing Data\n",
    "    \n",
    "\n",
    "- **[Formatting the Data for Decision Trees](#format-the-data)**\n",
    "    - Split data into Dependent and Independent Variables\n",
    "    - One-Hot-Encoding\n",
    "    \n",
    "\n",
    "- **[Building a Preliminary Classification Tree](#build-tree)**\n",
    "\n",
    "- **[Using Cost Complexity Pruning](#prune-tree)**\n",
    "    - Visualize Alpha\n",
    "    - Cross Validation For Finding the Best Alpha\n",
    "\n",
    "\n",
    "- **[Building, Drawing, Interpreting and Evaluating the Final Classification Tree](#draw-tree)**\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you are already know the basics of coding in **Python** and are familiar with the theory behind **Classification Trees**, **Cost Complexity Pruning**, **Cross Validation** and **Confusion Matrices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Import the modules that will do all the work\n",
    "\n",
    "The very first thing we do is load in a bunch of python modules. Python, itself, just gives us a basic programming language. These modules give us extra functionality to import the data, clean it up and format it, and then build, evaluate and draw the classification tree.\n",
    "\n",
    "NOTE: You will need Python 3 and have at least these versions for each of the following modules:\n",
    "\n",
    "    pandas >= 0.25.1\n",
    "    numpy >= 1.17.2\n",
    "    sklearn >= 0.22.1\n",
    "\n",
    "If you installed Python 3 with Anaconda can check which version you have with the command: conda list. If, for example, your version of scikit-learn is older than 0.22.1, then the easiest thing to do is just update all of your Anaconda packages with the following command: conda update --all. However, if only want to update scikit-learn, then you can run this command: conda install scikit-learn=0.22.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # load and manipulate data and for One-Hot Encoding\n",
    "import numpy as np # calculate the mean and standard deviation\n",
    "import matplotlib.pyplot as plt # drawing graphs\n",
    "from sklearn.tree import DecisionTreeClassifier # a classification tree\n",
    "from sklearn.tree import plot_tree # draw a classification tree\n",
    "from sklearn.model_selection import train_test_split # split  data into training and testing sets\n",
    "from sklearn.model_selection import cross_val_score # cross validation\n",
    "from sklearn.metrics import confusion_matrix # creates a confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2: Import the data\n",
    "\n",
    "Now we load in a dataset from the UCI Machine Learning Repository. Specifically, we are going to use the Heart Disease Dataset. This dataset will allow us to predict if someone has heart disease based on their sex, age, blood pressure and a variety of other metrics.\n",
    "\n",
    "NOTE: When pandas (pd) reads in data, it returns a data frame, which is a lot like a spreadsheet. The data are organized in rows and columns and each row can contain a mixture of text and numbers. The standard variable name for a data frame is the initials df, and that is what we will use here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data into a data frame called df, let's look at the first five rows using the head() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that instead of nice column names, we just have column numbers.  Since nice column names would make it easier to know how to format the data, let's replace the column numbers with the following column names:\n",
    "- **age**,\n",
    "- **sex**,\n",
    "- **cp**, chest pain\n",
    "- **restbp**, resting blood pressure (in mm Hg)\n",
    "- **chol**, serum cholesterol in mg/dl\n",
    "- **fbs**, fasting blood sugar\n",
    "- **restecg**, resting electrocardiographic results\n",
    "- **thalach**,  maximum heart rate achieved\n",
    "- **exang**, exercise induced angina\n",
    "- **oldpeak**, ST depression induced by exercise relative to rest\n",
    "- **slope**, the slope of the peak exercise ST segment.\n",
    "- **ca**, number of major vessels (0-3) colored by fluoroscopy\n",
    "- **thal**, this is short of thalium heart scan.\n",
    "- **hd**, diagnosis of heart disease, the predicted attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\n",
    "    'age',\n",
    "    'sex',\n",
    "    'cp',\n",
    "    'restbp',\n",
    "    'chol',\n",
    "    'fbs',\n",
    "    'restecg',\n",
    "    'thalach',\n",
    "    'exang',\n",
    "    'oldpeak',\n",
    "    'slope',\n",
    "    'ca',\n",
    "    'thal',\n",
    "    'hd'\n",
    "]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have replaced the column numbers with nice, easy to remember names. Now that we have the data in a data frame called df, we are ready to identify and deal with Missing Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"identify-and-deal-with-missing-data\"></a>\n",
    "# Task 3: Missing Data Part 1: Identifying Missing Data\n",
    "Unfortunately, the biggest part of any data analysis project is making sure that the data is correctly formatted and fixing it when it is not. The first part of this process is dealing with **Missing Data**.\n",
    "\n",
    "**Missing Data** is simply a blank space or surrogate value that indicates that we failed to collect data for one of the features. For example, if we forgot to ask someone's age, or forgot to write it down, then we would have a blank space in the dataset for that person's **age**.\n",
    "\n",
    "There are two main ways to deal with missing data:\n",
    "1. We can remove the rows that contain missing data from the dataset. This is relatively easy to do, but it wastes all of the other values that we collected. How a big of a waste this is depends on how important this missing value is for classification. For example, if we are missing a value for **age**, and **age** is not useful for classifying if people have heart disease or not, then it would be a shame to throw out all of someone's data just because we do not have their **age**.\n",
    "2. We can **impute** the values that are missing. In this context **impute** is just a fancy way of saying \"we can make an educated guess about about what the value should be\". Continuing our example where we are missing a value for **age**, instead of throwing out the entire row of data, we can fill the missing value with the average age or the median age, or use some other, more sophisticated approach, to guess at an appropriate value.\n",
    "\n",
    "In this section, we'll focus on identifying missing values in the dataset and dealing with them. \n",
    "\n",
    "First, let's see what sort of data is in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that that they are almost all `float64`, however, two columns, **ca** and **thal**, have the `object` type and one column, **hd** has `int64`.\n",
    "\n",
    "The fact that the **ca** and **thal** columns have `object` data types suggests there is something funny going on in them. `object` datatypes are used when there are mixtures of things, like a mixture of numbers and letters. In theory, both **ca** and **thal** should just have a few values representing different categories, so let's investigate what's going on by printing out their unique values. We'll start with **ca**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ca contains numbers (0.0, 3.0, 2.0 and 1.0) and questions marks (?). The numbers represent the number of blood vessels that we lit up by fluoroscopy and the question marks represent missing data.\n",
    "\n",
    "Now let's look at the unique values in **thal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, **thal** also contains a mixture of numbers, representing the different diagnoses from the thalium heart scan, and question marks, which represent missing values.\n",
    "\n",
    "----\n",
    "\n",
    "# Task 4: Missing Data Part 2: Dealing With Missing Data\n",
    "\n",
    "Since scikit-learn's classification trees do not support datasets with missing values, we need to figure out what to do these question marks. We can either delete these patients from the training dataset, or impute values for the missing data. First let's see how many rows contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c625d1d8f6ff9ea7f7fe158296eceb5f8007475ba92336ccf1402deb49436667"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
